{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c489d89c-c7c0-4926-bc53-f27e995d8cec",
   "metadata": {
    "id": "c489d89c-c7c0-4926-bc53-f27e995d8cec"
   },
   "source": [
    "## Information and Entropy\n",
    "\n",
    "Entropy is a concept from information theory designed to describe amount of uncertainty we have with a random variable in the given probability distribution. The idea is the more uncertain we are, the less information we have. To find entropy in essence we need to calculate average amount of information we need to describe a random variable. So, how would we measure information? We use \"bits\". To see how this works, let's look at two examples.\n",
    "\n",
    "#### Example 1\n",
    "Supppose we have the following distribution:\n",
    "\n",
    "$$Pr(x) = \\begin{cases}\n",
    "          0.25 & x =A \\\\\n",
    "          0.25 & x =B \\\\\n",
    "          0.25 & x =C \\\\\n",
    "          0.25 & x =D \\\\\n",
    "       \\end{cases}\n",
    "$$\n",
    "\n",
    "Given a random $x$, how many questions can we ask to determine what it equals to?\n",
    "Since all probabilties are the same, the most efficient way of doing it is to ask two questions:\n",
    "\n",
    "1. Is it A or B?\n",
    "\n",
    "2. If the answer was \"Yes\", then we ask: Is it A?; If the answer was \"No\", we ask: Is it C?\n",
    "\n",
    "So, we always ask two questions, therefore the average number of questions asked is two as well. And so, we say that the entropy here is 2 bits.\n",
    "\n",
    "#### Example 2\n",
    "\n",
    "Suppose now our probabilities are not the same:\n",
    "\n",
    "$$Pr(x) = \\begin{cases}\n",
    "          0.5 & x =A \\\\\n",
    "          0.25 & x =B \\\\\n",
    "          0.125 & x =C \\\\\n",
    "          0.125 & x =D \\\\\n",
    "       \\end{cases}\n",
    "$$\n",
    "\n",
    "We can do the same as in the example 1, but can we do better? Well, Since the probability $Pr(x=A)$ is already a half, $A$ will appear very often, so let's ask our questions in the following way:\n",
    "\n",
    "1. Is it A?\n",
    "\n",
    "2. If the answer was \"No\", we ask: Is it B?\n",
    "\n",
    "3. If the answer was \"No\", we ask: Is it C?\n",
    "\n",
    "On the first glance, it may seem like we are asking more questions now, but it depends. If $x=A$ for example, we are asking only 1 question. So what is the average number of questions we are asking? To find this, we just need to calculate the Expected Value of $x$. Let $N(x)$ denote number of questions we are asking to get to $x$ (so $N(A)=1$ and $N(C)=3$ for example)\n",
    "\n",
    "$$E(x)=\\sum_{x=A, B, C, D } Pr(x) \\cdot N(x)=0.5 \\cdot 1+0.25\\cdot 2+0.125\\cdot 3+0.125\\cdot 3=1.75$$\n",
    "\n",
    "So, on average we are asking less than two questions and we say that the entropy for this distribution is 1.75 bits.\n",
    "\n",
    "In the example 2, we have less entropy, because we have more predictive power than in the example 1.\n",
    "\n",
    "Let's formalize our calculation. To find $N(x)$ notice that it depends on $Pr(x)$. The lower the probability, the more questions we need to ask and since each question has binary answer, we need to look at the powers of 2:\n",
    "\n",
    "$$N(x)=\\log_2\\left(\\frac{1}{Pr(x)}\\right)$$\n",
    "\n",
    "So, suppose we have some probability distribution $P$ in which $P(i)=p_i$, then the entropy is:\n",
    "\n",
    "$$H(P)=\\sum_{i}p_i\\cdot\\log_2(1/p_i) = - \\sum_{i}p_i\\cdot\\log_2(p_i)$$\n",
    "\n",
    "Finally, we will only care about entropy as a relative quantity and so we can replace $\\log_2()$ with $\\ln()$ as they differ only by a constant:\n",
    "\n",
    "$$H(P)= - \\sum_{i}p_i\\cdot\\ln(p_i)$$\n",
    "\n",
    "### Example 3\n",
    "\n",
    "Suppose I have a bag with 10 blue and 10 red marbles and I want to pick one at random, then I can't really predict what kind of marble I will get, but if the bag had 19 red marbles and 1 blue marble, then I should be expecting to get a red marble. So in the first case I should have have larger entropy.\n",
    "\n",
    "In our first bag case, entropy is\n",
    "\n",
    "$$H= -\\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right)-\\frac{1}{2}\\ln\\left(\\frac{1}{2}\\right) \\approx 0.6931$$\n",
    "\n",
    "And in the second case:\n",
    "\n",
    "$$H=-\\frac{1}{20}\\ln\\left(\\frac{1}{20}\\right)-\\frac{19}{20}\\ln\\left(\\frac{19}{20}\\right) \\approx 0.198514$$\n",
    "\n",
    "\n",
    "Note: If we $p_k=0$, we would get $0 \\cdot \\ln(0)$ which is undefined. However, since 0 probability is perfect certainty, we set this to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fBXQnsJ2I2E",
   "metadata": {
    "id": "1fBXQnsJ2I2E"
   },
   "source": [
    "## Cross-Entropy and KL-Divergence\n",
    "\n",
    "\n",
    "Suppose I have two distributions from examples 1 and 2. We will think of them as some 4-sided dice. I will denote the distribution from the first example as $Q$ and the other as $P$. I would like to compare these two distributions. To do that I will look at the probabilities of getting specific samples. In other words, suppose I roll die $P$ 16 times, then we would expect to get 8 $A$'s, 4 $B$'s , 2 $C$'s and 2 $D$'s. (In fact, if we roll the die $N$ times, then the expected amount of let's say $A$'s is $N \\cdot Pr(A) = N \\cdot 0.5$.)\n",
    "\n",
    "Suppose we get these values in this specific order. Then the probability of getting this sequence is\n",
    "\n",
    "$$Prob=0.5^8 \\cdot 0.25^4 \\cdot 0.125^2 \\cdot 0.125^2$$\n",
    "\n",
    "This value is very tiny, and as $N$ grows, it gets even smaller. To fix this we will take $N$-th root to normalize it with respect to the sample size. Since the powers in the above calculations are of the form $N\\cdot Pr$, this root will eliminate $N$'s from the power, leaving just the probability.\n",
    "\n",
    "The second problem is that our numbers are still quite small and we are multiplying them which makes it even worse. Usual way to fix it is to take logarithm. We will take negative logarithm to keep our answer positive:\n",
    "\n",
    "$$-\\ln\\left(Prob^{1/N}\\right)=-\\frac{8}{16}\\ln(0.5)-\\frac{4}{16}\\ln(0.25)-\\frac{2}{16}\\ln(0.125)-\\frac{2}{16}\\ln(0.125)$$\n",
    "\n",
    "$$=-0.5\\ln(0.5)-0.25\\ln(0.25)-0.125\\ln(0.125)-0.125\\ln(0.125) \\approx 1.213$$\n",
    "\n",
    "Notice what we got is just entropy of $P$: $H(P)=-\\sum_i p_i \\ln(p_i)$. However, in this case I will think of this as cross-entropy of $P$ with respect to itself (I am using specific distribution to get expected distribution in my sample and I am using specific distribution to calculate the probability of getting that sample. It just happens to be the same distribution.)\n",
    "\n",
    "Now, what if we look for probability of getting the same sample but using die $Q$? The closer the $Q$ is to $P$ the closer the probabilites should be to the ones we got with a die $P$. So, we calculate:\n",
    "\n",
    "$$Prob=0.25^8 \\cdot 0.25^4 \\cdot 0.25^2 \\cdot 0.25^2$$\n",
    "\n",
    "Performing the same normalization:\n",
    "\n",
    "$$-\\ln\\left(Prob^{1/N}\\right)=-0.5\\ln(0.25)-0.25\\ln(0.25)-0.125\\ln(0.25)-0.125\\ln(0.25) \\approx 1.386$$\n",
    "\n",
    "What we got is a cross-entropy of $P$ relative to $Q$. To generalize this, let $p_i$'s be probabilties from $P$ and $q_i$'s be probabilities from $Q$, then the cross-entropy of $P$ relative to $Q$ is equal to\n",
    "\n",
    "$$H(P,Q)=-\\sum_i p_i \\ln(q_i)$$\n",
    "\n",
    "Few properties of cross-entropy:\n",
    "\n",
    "1. $H(P,P)=H(P)$\n",
    "\n",
    "2. $H(P,Q) \\geq H(P) \\geq 0$\n",
    "\n",
    "3. In general, $H(P,Q) \\neq H(Q,P)$\n",
    "\n",
    "\n",
    "Using these cross-entropies we can measure how far $Q$ is from $P$ by subtraction:\n",
    "\n",
    "$$D_{KL}(P||Q)= H(P,Q)-H(P)$$\n",
    "\n",
    "This is called KL-Divergence. So, in our example,\n",
    "\n",
    "$$D_{KL}(P||Q) \\approx 1.386- 1.213=0.173$$\n",
    "\n",
    "Properties of KL-Divergence:\n",
    "\n",
    "1. $D_{KL}(P||Q) \\geq 0$\n",
    "\n",
    "2. $D_{KL}(P||P) = 0$\n",
    "\n",
    "3. In general, $D_{KL}(P||Q) \\neq D_{KL}(Q||P) $\n",
    "\n",
    "### Example 4\n",
    "\n",
    "Suppose we have a third die $R$ with the following likelihoods:\n",
    "\n",
    "$$Pr(x) = \\begin{cases}\n",
    "          0.1 & x =A \\\\\n",
    "          0.1 & x =B \\\\\n",
    "          0.4 & x =C \\\\\n",
    "          0.4 & x =D \\\\\n",
    "       \\end{cases}\n",
    "$$\n",
    "\n",
    "If you look carefully, it should be quite obvious that $R$ is much further from $P$ than $Q$ was. So let's see that by calculating $D_{KL}(P||R)$\n",
    "\n",
    "$$H(P, R)=-0.5\\ln(0.1)-0.25\\ln(0.1)-0.125\\ln(0.4)-0.125\\ln(0.4) \\approx 1.956$$\n",
    "\n",
    "And so, $$D_{KL}(P||R) \\approx 1.956- 1.213=0.743$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b3b12",
   "metadata": {
    "id": "c11b3b12"
   },
   "source": [
    "## Cross-Entropy Loss\n",
    "\n",
    "Suppose we have some labeled training data and we have some classification model parametrized with some parameter $\\theta$. Then, given a data point $x$, our model produces a prediction $\\hat{y}$. This prediction is a vector with number of entries equal to number of classes. Each entry is the probability that our data point belongs to a corresponding class. For example, if\n",
    "$$\\hat{y}=\\begin{bmatrix} 0.2 \\\\ 0.3 \\\\0.5 \\end{bmatrix}$$\n",
    "Then the probability that $x$ belongs to class 0 is 0.2, the probability that $x$ belongs to class 1 is 0.3, and the probability that $x$ belongs to class 2 is 0.5. So, we get a distribution for $x$. I will denote this as $$ \\hat{P}(x|\\theta)=\\begin{bmatrix} \\hat{p}_1 \\\\ \\hat{p}_2 \\\\\\hat{p}_3 \\end{bmatrix}$$\n",
    "\n",
    "However, our data is labeled, so we do have the actual label $y$ for $x$. This is also a probability vector. However, it contains a single 1 in the position corresponding to a true class of $x$, and it has zeros everywhere else. Nevertheless, it is still a distribution, and I will denote it as\n",
    "\n",
    "$$P(x)=\\begin{bmatrix} {p}_1 \\\\ {p}_2 \\\\{p}_3 \\end{bmatrix}$$ Note, that it doesn't depend on $\\theta$ since it doesn't come from our training model.\n",
    "\n",
    "We would like our model to predict the correct class, so we want $ \\hat{P}(x|\\theta)$ to be close to $P(x)$. We can use KL-divergence to see how close two distributions are and try to minimize it.\n",
    "\n",
    "$$D_{KL}\\left(P(x) || \\hat{P}(x|\\theta)\\right)=H\\left(P(x), \\hat{P}(x|\\theta)\\right) - H\\left(P(x)\\right)$$\n",
    "\n",
    "Two things to note here:\n",
    "\n",
    "1. We can not reverse distributions inside $D_{KL}$, since then we would have to use entropy of $\\hat{P}$. However, we don't want this entropy to affect our optimization.\n",
    "\n",
    "2. Since $H(P(x))$ doesn't depend on $\\theta$, our optimization doesn't depend on this term. In other words, thr parameter $\\theta$ that minimizes KL-divergence will also minimize cross-entropy:\n",
    "\n",
    "$$\\mathop{\\arg\\min}_{\\theta} D_{KL}\\left(P(x) || \\hat{P}(x|\\theta)\\right) = \\mathop{\\arg\\min}_{\\theta}H\\left(P(x), \\hat{P}(x|\\theta)\\right)$$\n",
    "\n",
    "So, we now define the Cross-Entropy Loss function for a single datapoint $x$ as follows :\n",
    "\n",
    "$$CELoss(x)=H\\left(P(x), \\hat{P}(x|\\theta)\\right)=-\\sum_i p_i\\ln(\\hat{p}_i)$$\n",
    "\n",
    "And for the whole data set, as before, we add losses for each point in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RshaMBH2HOqx",
   "metadata": {
    "id": "RshaMBH2HOqx"
   },
   "source": [
    "## Relation of CELoss to NLLoss\n",
    "\n",
    "When we have only two classes, then\n",
    "\n",
    "$$P(x)=\\begin{bmatrix} p \\\\ 1-p  \\end{bmatrix}, \\hat{P}(x)=\\begin{bmatrix} \\hat{p} \\\\ 1-\\hat{p}  \\end{bmatrix}$$\n",
    "\n",
    "Then,\n",
    "\n",
    "$$CELoss(x)=-p\\ln(\\hat{p})-(1-p)\\ln(1-\\hat{p})=NLLoss(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d0cb4b-b5d7-47da-b817-de3b2e4b8c44",
   "metadata": {},
   "source": [
    "### Problem Set 1\n",
    "\n",
    "In Problem Set 1, use logarithm base 2 for calculating entropies.\n",
    "\n",
    "1. Suppose we have the following distribution $A$:\n",
    "    $$Pr(x) = \\begin{cases}\n",
    "          0.5 & x =A \\\\\n",
    "          0.3 & x =B \\\\\n",
    "          0.1 & x =C \\\\\n",
    "          0.1 & x =D \\\\\n",
    "       \\end{cases}$$\n",
    "\n",
    "    a. Compute Entropy for this distribution.\n",
    "\n",
    "    b. Suppose we get a random letter and we ask \"Is it A?\". If the answer is \"Yes\", then we are left with only one possibility: it is an A. So in this case, we have a new distribution that just says $P(A)=1$. What is the entropy of this distribution?\n",
    "\n",
    "    c. Suppose the answer to the \"Is it A?\" was a \"No\". What is the new distribution? and what is the entropy of it?  \n",
    "\n",
    "    d. Since the probability of \"Yes\" or \"No\" was 50% each in this case, we can find expected entropy of a new distrubution by just computing the average of the two new entropies. Find this expected entropy. Then subtract it from original entropy. \n",
    "\n",
    "    e. What do you think the final answer in part d. represents?\n",
    "\n",
    "\n",
    "\n",
    "2. Repeat problem 1, but in part b. ask \"Is is B or C?\" instead. Note that in part d. to find expected entropy you would need use the fact that the probability of \"Yes\" is 0.4 and probability of \"No\" is 0.6.\n",
    "\n",
    "3. In the above problems, which question was better \"Is it A?\" or \"Is it B or C?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e83703a-2052-48db-be5f-035fe8d6bb3a",
   "metadata": {},
   "source": [
    "### Problem set 2\n",
    "\n",
    "Consider two distributions $B$ and $C$:\n",
    "\n",
    "$$Pr(x) = \\begin{cases}\n",
    "      0.55 & x =A \\\\\n",
    "      0.25 & x =B \\\\\n",
    "      0.05 & x =C \\\\\n",
    "      0.15 & x =D \\\\\n",
    "   \\end{cases}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$Pr(x) = \\begin{cases}\n",
    "      0.4 & x =A \\\\\n",
    "      0.3 & x =B \\\\\n",
    "      0.15 & x =C \\\\\n",
    "      0.15 & x =D \\\\\n",
    "   \\end{cases}$$\n",
    "\n",
    "1. Which of these two distributions is \"closer\" to the distribution $A$ from Set 1?\n",
    "\n",
    "2. Generate three random sets of size 1000 using each of the three distributions. Calculate KL-Divergence between $A$ and $B$, and between $A$ and $C$. Do we get simmilar results?\n",
    "\n",
    "Note:\n",
    "You can generate sets using `np.random.choice`. For example set A is `[np.random.choice(np.arange(1, 5), p=[0.5, 0.3, 0.1, 0.1]) for i in range(1000)]`\n",
    "\n",
    "You can compute KL-divergence between sets A and B using `entropy(A,B)` from `from scipy.stats import entropy`. And if you use `entropy(A)`, you get entropy of A. Both use natural log. If you wish to use base two log, add `base=2` inside entropy command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689fcdac-6558-49cd-b8a3-eb71f1b070a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
